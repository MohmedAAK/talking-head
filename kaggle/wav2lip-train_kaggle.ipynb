{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ae9980",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T09:58:53.826577Z",
     "iopub.status.busy": "2023-11-02T09:58:53.824953Z",
     "iopub.status.idle": "2023-11-02T09:58:53.830081Z",
     "shell.execute_reply": "2023-11-02T09:58:53.829110Z"
    },
    "papermill": {
     "duration": 0.054963,
     "end_time": "2023-11-02T09:58:53.830920",
     "exception": false,
     "start_time": "2023-11-02T09:58:53.775957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995b0cd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T09:58:53.906887Z",
     "iopub.status.busy": "2023-11-02T09:58:53.906018Z",
     "iopub.status.idle": "2023-11-02T09:58:53.911764Z",
     "shell.execute_reply": "2023-11-02T09:58:53.910957Z"
    },
    "papermill": {
     "duration": 0.047459,
     "end_time": "2023-11-02T09:58:53.911940",
     "exception": false,
     "start_time": "2023-11-02T09:58:53.864481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python preprocess.py --data_root \"./mvlrs_v1/main\" --preprocessed_root \"./lrs2_preprocessed\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b17918ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T09:58:54.079226Z",
     "iopub.status.busy": "2023-11-02T09:58:54.078452Z",
     "iopub.status.idle": "2023-11-02T09:58:54.085762Z",
     "shell.execute_reply": "2023-11-02T09:58:54.085154Z",
     "shell.execute_reply.started": "2021-07-30T15:10:55.205557Z"
    },
    "papermill": {
     "duration": 0.137186,
     "end_time": "2023-11-02T09:58:54.085935",
     "exception": false,
     "start_time": "2023-11-02T09:58:53.948749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/wave2lip/wav2lip_homework\n"
     ]
    }
   ],
   "source": [
    "%cd ../input/wave2lip/wav2lip_homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f06eeec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T09:58:54.169505Z",
     "iopub.status.busy": "2023-11-02T09:58:54.168892Z",
     "iopub.status.idle": "2023-11-02T09:58:55.555484Z",
     "shell.execute_reply": "2023-11-02T09:58:55.554824Z",
     "shell.execute_reply.started": "2021-07-30T15:10:59.610288Z"
    },
    "papermill": {
     "duration": 1.427886,
     "end_time": "2023-11-02T09:58:55.555646",
     "exception": false,
     "start_time": "2023-11-02T09:58:54.127760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv_block = nn.Sequential(\n",
    "                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n",
    "                            nn.BatchNorm2d(cout)\n",
    "                            )\n",
    "        self.act = nn.ReLU()\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        if self.residual:\n",
    "            out += x\n",
    "        return self.act(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2938fdac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T09:58:55.638709Z",
     "iopub.status.busy": "2023-11-02T09:58:55.634845Z",
     "iopub.status.idle": "2023-11-02T09:58:55.641270Z",
     "shell.execute_reply": "2023-11-02T09:58:55.640675Z",
     "shell.execute_reply.started": "2021-07-30T15:11:04.53338Z"
    },
    "papermill": {
     "duration": 0.055361,
     "end_time": "2023-11-02T09:58:55.641393",
     "exception": false,
     "start_time": "2023-11-02T09:58:55.586032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SyncNet_color(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SyncNet_color, self).__init__()\n",
    "        \n",
    "\n",
    "        self.face_encoder = nn.Sequential(\n",
    "            Conv2d(15, 32, kernel_size=(7, 7), stride=1, padding=3),\n",
    "\n",
    "            Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=0),\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
    "\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
    "\n",
    "    def forward(self, audio_sequences, face_sequences): # audio_sequences := (B, dim, T)\n",
    "        \n",
    "        #########################TODO#######################\n",
    "        # 正向传播\n",
    "        face_embedding = self.face_encoder(face_sequences)\n",
    "        audio_embedding = self.audio_encoder(audio_sequences)\n",
    "\n",
    "        audio_embedding = audio_embedding.view(audio_embedding.size(0), -1)\n",
    "        face_embedding = face_embedding.view(face_embedding.size(0), -1)\n",
    "\n",
    "        audio_embedding = F.normalize(audio_embedding, p=2, dim=1)\n",
    "        face_embedding = F.normalize(face_embedding, p=2, dim=1)\n",
    "\n",
    "\n",
    "        return audio_embedding, face_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc23e5ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T09:58:55.707818Z",
     "iopub.status.busy": "2023-11-02T09:58:55.707084Z",
     "iopub.status.idle": "2023-11-02T09:58:57.984519Z",
     "shell.execute_reply": "2023-11-02T09:58:57.983832Z",
     "shell.execute_reply.started": "2021-07-30T15:11:11.303526Z"
    },
    "papermill": {
     "duration": 2.312848,
     "end_time": "2023-11-02T09:58:57.984682",
     "exception": false,
     "start_time": "2023-11-02T09:58:55.671834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os.path import dirname, join, basename, isfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import SyncNet_color as SyncNet\n",
    "import audio\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import os, random, cv2, argparse\n",
    "from hparams import hparams, get_image_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6dae6",
   "metadata": {
    "papermill": {
     "duration": 0.030498,
     "end_time": "2023-11-02T09:58:58.045988",
     "exception": false,
     "start_time": "2023-11-02T09:58:58.015490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2.数据集的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a341da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T09:58:58.182158Z",
     "iopub.status.busy": "2023-11-02T09:58:58.181183Z",
     "iopub.status.idle": "2023-11-02T09:58:58.184634Z",
     "shell.execute_reply": "2023-11-02T09:58:58.185230Z",
     "shell.execute_reply.started": "2021-07-30T15:11:14.929415Z"
    },
    "papermill": {
     "duration": 0.108864,
     "end_time": "2023-11-02T09:58:58.185393",
     "exception": false,
     "start_time": "2023-11-02T09:58:58.076529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "global_step = 0 \n",
    "global_epoch = 0 \n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5 \n",
    "syncnet_mel_step_size = 16 \n",
    "data_root=\"/kaggle/input/wav2lippreprocessed/lrs2_preprocessed\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f9c11ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T09:58:58.266899Z",
     "iopub.status.busy": "2023-11-02T09:58:58.266020Z",
     "iopub.status.idle": "2023-11-02T09:58:58.268855Z",
     "shell.execute_reply": "2023-11-02T09:58:58.268343Z",
     "shell.execute_reply.started": "2021-07-30T15:18:29.73854Z"
    },
    "papermill": {
     "duration": 0.052787,
     "end_time": "2023-11-02T09:58:58.268993",
     "exception": false,
     "start_time": "2023-11-02T09:58:58.216206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, split):\n",
    "        self.all_videos = get_image_list(data_root, split)\n",
    "\n",
    "    def get_frame_id(self, frame):\n",
    "        return int(basename(frame).split('.')[0])\n",
    "\n",
    "    def get_window(self, start_frame):\n",
    "        start_id = self.get_frame_id(start_frame)\n",
    "        vidname = dirname(start_frame)\n",
    "\n",
    "        window_fnames = []\n",
    "        for frame_id in range(start_id, start_id + syncnet_T):\n",
    "            frame = join(vidname, '{}.jpg'.format(frame_id))\n",
    "            if not isfile(frame):\n",
    "                return None\n",
    "            window_fnames.append(frame)\n",
    "        return window_fnames\n",
    "\n",
    "    def crop_audio_window(self, spec, start_frame):\n",
    "        # num_frames = (T x hop_size * fps) / sample_rate\n",
    "        start_frame_num = self.get_frame_id(start_frame)\n",
    "        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n",
    "\n",
    "        end_idx = start_idx + syncnet_mel_step_size\n",
    "\n",
    "        return spec[start_idx : end_idx, :]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        return: x,mel,y\n",
    "        x: 五张嘴唇图片\n",
    "        mel：对应的语音的mel spectrogram\n",
    "        t：同步or不同步\n",
    "        \n",
    "        \"\"\"\n",
    "        while 1:\n",
    "            idx = random.randint(0, len(self.all_videos) - 1)\n",
    "            vidname = self.all_videos[idx]\n",
    "\n",
    "            img_names = list(glob(join(vidname, '*.jpg')))\n",
    "            if len(img_names) <= 3 * syncnet_T:\n",
    "                continue\n",
    "            img_name = random.choice(img_names)\n",
    "            wrong_img_name = random.choice(img_names)\n",
    "            while wrong_img_name == img_name:\n",
    "                wrong_img_name = random.choice(img_names)\n",
    "            \n",
    "            \n",
    "            #随机决定是产生负样本还是正样本\n",
    "            if random.choice([True, False]):\n",
    "                y = torch.ones(1).float()\n",
    "                chosen = img_name\n",
    "            else:\n",
    "                y = torch.zeros(1).float()\n",
    "                chosen = wrong_img_name\n",
    "\n",
    "            window_fnames = self.get_window(chosen)\n",
    "            if window_fnames is None:\n",
    "                continue\n",
    "\n",
    "            window = []\n",
    "            all_read = True\n",
    "            for fname in window_fnames:\n",
    "                img = cv2.imread(fname)\n",
    "                if img is None:\n",
    "                    all_read = False\n",
    "                    break\n",
    "                try:\n",
    "                    img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n",
    "                except Exception as e:\n",
    "                    all_read = False\n",
    "                    break\n",
    "\n",
    "                window.append(img)\n",
    "\n",
    "            if not all_read: continue\n",
    "\n",
    "            try:\n",
    "                wavpath = join(vidname, \"audio.wav\")\n",
    "                wav = audio.load_wav(wavpath, hparams.sample_rate)\n",
    "\n",
    "                orig_mel = audio.melspectrogram(wav).T\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n",
    "\n",
    "            if (mel.shape[0] != syncnet_mel_step_size):\n",
    "                continue\n",
    "\n",
    "            # H x W x 3 * T\n",
    "            x = np.concatenate(window, axis=2) / 255.\n",
    "            x = x.transpose(2, 0, 1)\n",
    "            x = x[:, x.shape[1]//2:]\n",
    "\n",
    "            x = torch.FloatTensor(x)\n",
    "            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n",
    "\n",
    "            return x, mel, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "920f4070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T09:58:58.335007Z",
     "iopub.status.busy": "2023-11-02T09:58:58.334362Z",
     "iopub.status.idle": "2023-11-02T09:58:58.589298Z",
     "shell.execute_reply": "2023-11-02T09:58:58.590240Z",
     "shell.execute_reply.started": "2021-07-30T15:18:33.470434Z"
    },
    "papermill": {
     "duration": 0.290821,
     "end_time": "2023-11-02T09:58:58.590550",
     "exception": false,
     "start_time": "2023-11-02T09:58:58.299729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 48, 96])\n",
      "torch.Size([1, 80, 16])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "ds=Dataset(\"train\")\n",
    "x,mel,t=ds[0]\n",
    "print(x.shape)\n",
    "print(mel.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "868d587d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T09:58:58.721662Z",
     "iopub.status.busy": "2023-11-02T09:58:58.721057Z",
     "iopub.status.idle": "2023-11-02T09:58:58.861564Z",
     "shell.execute_reply": "2023-11-02T09:58:58.860943Z",
     "shell.execute_reply.started": "2021-07-30T15:18:36.933382Z"
    },
    "papermill": {
     "duration": 0.204303,
     "end_time": "2023-11-02T09:58:58.861712",
     "exception": false,
     "start_time": "2023-11-02T09:58:58.657409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7b4478a1e8d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(mel[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a8dbd90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T09:58:58.956254Z",
     "iopub.status.busy": "2023-11-02T09:58:58.955346Z",
     "iopub.status.idle": "2023-11-02T09:58:59.067961Z",
     "shell.execute_reply": "2023-11-02T09:58:59.068442Z",
     "shell.execute_reply.started": "2021-07-30T15:18:56.039099Z"
    },
    "papermill": {
     "duration": 0.173294,
     "end_time": "2023-11-02T09:58:59.068615",
     "exception": false,
     "start_time": "2023-11-02T09:58:58.895321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7b447893b2d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[:3,:,:].transpose(0,2).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dd8a04d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T09:58:59.143666Z",
     "iopub.status.busy": "2023-11-02T09:58:59.143020Z",
     "iopub.status.idle": "2023-11-02T09:58:59.145848Z",
     "shell.execute_reply": "2023-11-02T09:58:59.145198Z",
     "shell.execute_reply.started": "2021-07-30T15:19:00.674203Z"
    },
    "papermill": {
     "duration": 0.042048,
     "end_time": "2023-11-02T09:58:59.145974",
     "exception": false,
     "start_time": "2023-11-02T09:58:59.103926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logloss = nn.BCELoss() \n",
    "def cosine_loss(a, v, y):\n",
    "    \"\"\"\n",
    "    a: audio_encoder的输出\n",
    "    v: video face_encoder的输出\n",
    "    y: 是否同步的真实值\n",
    "    \"\"\"\n",
    "    d = nn.functional.cosine_similarity(a, v)\n",
    "    loss = logloss(d.unsqueeze(1), y)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ccba028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T09:58:59.234499Z",
     "iopub.status.busy": "2023-11-02T09:58:59.233649Z",
     "iopub.status.idle": "2023-11-02T09:58:59.236445Z",
     "shell.execute_reply": "2023-11-02T09:58:59.235925Z",
     "shell.execute_reply.started": "2021-07-30T15:19:09.309393Z"
    },
    "papermill": {
     "duration": 0.055656,
     "end_time": "2023-11-02T09:58:59.236560",
     "exception": false,
     "start_time": "2023-11-02T09:58:59.180904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(device, model, train_data_loader, test_data_loader, optimizer,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    "    \n",
    "    while global_epoch < nepochs:\n",
    "        running_loss = 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, mel, y) in prog_bar:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x = x.to(device)\n",
    "\n",
    "            mel = mel.to(device)\n",
    "\n",
    "            a, v = model(mel, x)\n",
    "            y = y.to(device)\n",
    "\n",
    "            loss = cosine_loss(a, v, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            \n",
    "\n",
    "            global_step += 1\n",
    "            cur_session_steps = global_step - resumed_step\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if global_step == 1 or global_step % checkpoint_interval == 0:\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n",
    "\n",
    "            if global_step % hparams.syncnet_eval_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    eval_model(test_data_loader, global_step, device, model, checkpoint_dir)\n",
    "\n",
    "            prog_bar.set_description('Epoch: {} Loss: {}'.format(global_epoch, running_loss / (step + 1)))\n",
    "\n",
    "        global_epoch += 1\n",
    "\n",
    "def eval_model(test_data_loader, global_step, device, model, checkpoint_dir):\n",
    "    eval_steps = 1400\n",
    "    print('Evaluating for {} steps'.format(eval_steps))\n",
    "    losses = []\n",
    "    while 1:\n",
    "        for step, (x, mel, y) in enumerate(test_data_loader):\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            # Transform data to CUDA device\n",
    "            x = x.to(device)\n",
    "\n",
    "            mel = mel.to(device)\n",
    "\n",
    "            a, v = model(mel, x)\n",
    "            y = y.to(device)\n",
    "\n",
    "            loss = cosine_loss(a, v, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if step > eval_steps: break\n",
    "\n",
    "        averaged_loss = sum(losses) / len(losses)\n",
    "        print(averaged_loss)\n",
    "\n",
    "        return\n",
    "\n",
    "latest_checkpoint_path = ''\n",
    "def save_checkpoint(model, optimizer, step, checkpoint_dir, epoch):\n",
    "    global latest_checkpoint_path\n",
    "    \n",
    "    checkpoint_path = join(\n",
    "        checkpoint_dir, \"checkpoint_step{:09d}.pth\".format(global_step))\n",
    "    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n",
    "    torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer_state,\n",
    "        \"global_step\": step,\n",
    "        \"global_epoch\": epoch,\n",
    "    }, checkpoint_path)\n",
    "    latest_checkpoint_path = checkpoint_path\n",
    "    print(\"Saved checkpoint:\", checkpoint_path)\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, reset_optimizer=False):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    global_step = checkpoint[\"global_step\"]\n",
    "    global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d70f9af8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T09:58:59.314151Z",
     "iopub.status.busy": "2023-11-02T09:58:59.313469Z",
     "iopub.status.idle": "2023-11-02T12:16:33.973226Z",
     "shell.execute_reply": "2023-11-02T12:16:33.974128Z",
     "shell.execute_reply.started": "2021-07-30T15:19:14.2154Z"
    },
    "papermill": {
     "duration": 8254.703563,
     "end_time": "2023-11-02T12:16:33.974378",
     "exception": false,
     "start_time": "2023-11-02T09:58:59.270815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params 16435072\n",
      "Load checkpoint from: /kaggle/input/wav2lip24epoch/expert_checkpoints/checkpoint_step000060000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Loss: 0.48332203482562125: : 2865it [20:44,  2.30it/s]\n",
      "Epoch: 21 Loss: 0.4821069194332259: : 2865it [19:16,  2.48it/s]\n",
      "Epoch: 22 Loss: 0.47878145759122326: : 2865it [19:23,  2.46it/s]\n",
      "Epoch: 23 Loss: 0.47948348685632064: : 1404it [09:41,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/expert_checkpoints/checkpoint_step000070000.pth\n",
      "Evaluating for 1400 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Loss: 0.4799394829313892: : 1407it [10:04,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4412049684016144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Loss: 0.4790076069130739: : 2865it [20:04,  2.38it/s]\n",
      "Epoch: 24 Loss: 0.4703784715338735: : 2865it [19:39,  2.43it/s]\n",
      "Epoch: 25 Loss: 0.4672579793561816: : 2865it [19:05,  2.50it/s]\n",
      "Epoch: 26 Loss: 0.4646043508035426: : 2809it [18:40,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/expert_checkpoints/checkpoint_step000080000.pth\n",
      "Evaluating for 1400 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Loss: 0.4646491017185976: : 2810it [19:00,  6.47s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.432718401446062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Loss: 0.4643614772414662: : 2865it [19:12,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"/kaggle/working/expert_checkpoints/\"\n",
    "checkpoint_path = '/kaggle/input/wav2lip24epoch/expert_checkpoints/checkpoint_step000060000.pth'\n",
    "\n",
    "\n",
    "if not os.path.exists(checkpoint_dir): os.mkdir(checkpoint_dir)\n",
    "\n",
    "# Dataset and Dataloader setup\n",
    "train_dataset = Dataset('train')\n",
    "test_dataset = Dataset('val')\n",
    "\n",
    "train_data_loader = data_utils.DataLoader(\n",
    "    train_dataset, batch_size=hparams.batch_size, shuffle=True,\n",
    "    num_workers=hparams.num_workers)\n",
    "\n",
    "test_data_loader = data_utils.DataLoader(\n",
    "    test_dataset, batch_size=hparams.batch_size,\n",
    "    num_workers=8)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Model\n",
    "\n",
    "model = SyncNet().to(device)\n",
    "print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                       lr=1e-5)\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=True)\n",
    "\n",
    "train(device, model, train_data_loader, test_data_loader, optimizer,\n",
    "      checkpoint_dir=checkpoint_dir,\n",
    "      checkpoint_interval=hparams.syncnet_checkpoint_interval,\n",
    "      nepochs=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92e150d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T12:16:59.921686Z",
     "iopub.status.busy": "2023-11-02T12:16:59.921069Z",
     "iopub.status.idle": "2023-11-02T12:16:59.923933Z",
     "shell.execute_reply": "2023-11-02T12:16:59.923313Z",
     "shell.execute_reply.started": "2021-07-25T08:00:59.2366Z"
    },
    "papermill": {
     "duration": 12.851627,
     "end_time": "2023-11-02T12:16:59.924064",
     "exception": false,
     "start_time": "2023-11-02T12:16:47.072437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class nonorm_Conv2d(nn.Module): \n",
    "    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv_block = nn.Sequential(\n",
    "                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n",
    "                            )\n",
    "        self.act = nn.LeakyReLU(0.01, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        return self.act(out)\n",
    "\n",
    "class Conv2dTranspose(nn.Module):\n",
    "    def __init__(self, cin, cout, kernel_size, stride, padding, output_padding=0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(cin, cout, kernel_size, stride, padding, output_padding),\n",
    "                            nn.BatchNorm2d(cout)\n",
    "                            )\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_block(x)\n",
    "        return self.act(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b13cf592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T12:17:25.887127Z",
     "iopub.status.busy": "2023-11-02T12:17:25.886324Z",
     "iopub.status.idle": "2023-11-02T12:17:26.081840Z",
     "shell.execute_reply": "2023-11-02T12:17:26.081155Z",
     "shell.execute_reply.started": "2021-07-25T08:00:04.850039Z"
    },
    "papermill": {
     "duration": 13.21889,
     "end_time": "2023-11-02T12:17:26.081978",
     "exception": false,
     "start_time": "2023-11-02T12:17:12.863088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Wav2Lip(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wav2Lip, self).__init__()\n",
    "\n",
    "        self.face_encoder_blocks = nn.ModuleList([\n",
    "            nn.Sequential(Conv2d(6, 16, kernel_size=7, stride=1, padding=3)), # 96,96\n",
    "\n",
    "            nn.Sequential(Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 48,48\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(32, 64, kernel_size=3, stride=2, padding=1),    # 24,24\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(64, 128, kernel_size=3, stride=2, padding=1),   # 12,12\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(128, 256, kernel_size=3, stride=2, padding=1),       # 6,6\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True)),\n",
    "\n",
    "            nn.Sequential(Conv2d(256, 512, kernel_size=3, stride=2, padding=1),     # 3,3\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\n",
    "            \n",
    "            nn.Sequential(Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\n",
    "\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n",
    "            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n",
    "\n",
    "        self.face_decoder_blocks = nn.ModuleList([\n",
    "            nn.Sequential(Conv2d(512, 512, kernel_size=1, stride=1, padding=0),),\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=1, padding=0), # 3,3\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),), # 6, 6\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(768, 384, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),), # 12, 12\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),), # 24, 24\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(320, 128, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),), # 48, 48\n",
    "\n",
    "            nn.Sequential(Conv2dTranspose(160, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n",
    "            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),),]) # 96,96\n",
    "\n",
    "        self.output_block = nn.Sequential(Conv2d(80, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(32, 3, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Sigmoid()) \n",
    "\n",
    "    def forward(self, audio_sequences, face_sequences):\n",
    "        # audio_sequences = (B, T, 1, 80, 16)\n",
    "        B = audio_sequences.size(0)\n",
    "\n",
    "        input_dim_size = len(face_sequences.size())\n",
    "        if input_dim_size > 4:\n",
    "            audio_sequences = torch.cat([audio_sequences[:, i] for i in range(audio_sequences.size(1))], dim=0)\n",
    "            face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\n",
    "\n",
    "        audio_embedding = self.audio_encoder(audio_sequences) # B, 512, 1, 1\n",
    "\n",
    "        feats = []\n",
    "        x = face_sequences\n",
    "        for f in self.face_encoder_blocks:\n",
    "            x = f(x)\n",
    "            feats.append(x)\n",
    "\n",
    "        x = audio_embedding\n",
    "        for f in self.face_decoder_blocks:\n",
    "            x = f(x)\n",
    "            try:\n",
    "                x = torch.cat((x, feats[-1]), dim=1)\n",
    "            except Exception as e:\n",
    "                print(x.size())\n",
    "                print(feats[-1].size())\n",
    "                raise e\n",
    "            \n",
    "            feats.pop()\n",
    "\n",
    "        x = self.output_block(x)\n",
    "\n",
    "        if input_dim_size > 4:\n",
    "            x = torch.split(x, B, dim=0) # [(B, C, H, W)]\n",
    "            outputs = torch.stack(x, dim=2) # (B, C, T, H, W)\n",
    "\n",
    "        else:\n",
    "            outputs = x\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f86ef11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T12:17:51.829719Z",
     "iopub.status.busy": "2023-11-02T12:17:51.828639Z",
     "iopub.status.idle": "2023-11-02T12:17:51.831761Z",
     "shell.execute_reply": "2023-11-02T12:17:51.831045Z",
     "shell.execute_reply.started": "2021-07-25T08:00:08.961755Z"
    },
    "papermill": {
     "duration": 13.003755,
     "end_time": "2023-11-02T12:17:51.831924",
     "exception": false,
     "start_time": "2023-11-02T12:17:38.828169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Wav2Lip_disc_qual(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Wav2Lip_disc_qual, self).__init__()\n",
    "\n",
    "        self.face_encoder_blocks = nn.ModuleList([\n",
    "            nn.Sequential(nonorm_Conv2d(3, 32, kernel_size=7, stride=1, padding=3)), # 48,96\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=2), # 48,48\n",
    "            nonorm_Conv2d(64, 64, kernel_size=5, stride=1, padding=2)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(64, 128, kernel_size=5, stride=2, padding=2),    # 24,24\n",
    "            nonorm_Conv2d(128, 128, kernel_size=5, stride=1, padding=2)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(128, 256, kernel_size=5, stride=2, padding=2),   # 12,12\n",
    "            nonorm_Conv2d(256, 256, kernel_size=5, stride=1, padding=2)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(256, 512, kernel_size=3, stride=2, padding=1),       # 6,6\n",
    "            nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1)),\n",
    "\n",
    "            nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=2, padding=1),     # 3,3\n",
    "            nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1),),\n",
    "            \n",
    "            nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\n",
    "            nonorm_Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\n",
    "\n",
    "        self.binary_pred = nn.Sequential(nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=0), nn.Sigmoid())\n",
    "        self.label_noise = .0\n",
    "\n",
    "    def get_lower_half(self, face_sequences):\n",
    "        return face_sequences[:, :, face_sequences.size(2)//2:]\n",
    "\n",
    "    def to_2d(self, face_sequences):\n",
    "        B = face_sequences.size(0)\n",
    "        face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\n",
    "        return face_sequences\n",
    "\n",
    "    def perceptual_forward(self, false_face_sequences):\n",
    "        false_face_sequences = self.to_2d(false_face_sequences)\n",
    "        false_face_sequences = self.get_lower_half(false_face_sequences)\n",
    "\n",
    "        false_feats = false_face_sequences\n",
    "        for f in self.face_encoder_blocks:\n",
    "            false_feats = f(false_feats)\n",
    "\n",
    "        false_pred_loss = F.binary_cross_entropy(self.binary_pred(false_feats).view(len(false_feats), -1), \n",
    "                                        torch.ones((len(false_feats), 1)).cuda())\n",
    "\n",
    "        return false_pred_loss\n",
    "\n",
    "    def forward(self, face_sequences):\n",
    "        face_sequences = self.to_2d(face_sequences)\n",
    "        face_sequences = self.get_lower_half(face_sequences)\n",
    "\n",
    "        x = face_sequences\n",
    "        for f in self.face_encoder_blocks:\n",
    "            x = f(x)\n",
    "\n",
    "        return self.binary_pred(x).view(len(x), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d726ae93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T12:18:17.646281Z",
     "iopub.status.busy": "2023-11-02T12:18:17.645616Z",
     "iopub.status.idle": "2023-11-02T12:18:17.648668Z",
     "shell.execute_reply": "2023-11-02T12:18:17.649112Z",
     "shell.execute_reply.started": "2021-07-25T08:00:11.621386Z"
    },
    "papermill": {
     "duration": 12.905371,
     "end_time": "2023-11-02T12:18:17.649267",
     "exception": false,
     "start_time": "2023-11-02T12:18:04.743896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: True\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('use_cuda: {}'.format(use_cuda))\n",
    "\n",
    "syncnet_T = 5\n",
    "syncnet_mel_step_size = 16\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, split):\n",
    "        self.all_videos = get_image_list(data_root, split)\n",
    "\n",
    "    def get_frame_id(self, frame):\n",
    "        return int(basename(frame).split('.')[0])\n",
    "\n",
    "    def get_window(self, start_frame):\n",
    "        start_id = self.get_frame_id(start_frame)\n",
    "        vidname = dirname(start_frame)\n",
    "\n",
    "        window_fnames = []\n",
    "        for frame_id in range(start_id, start_id + syncnet_T):\n",
    "            frame = join(vidname, '{}.jpg'.format(frame_id))\n",
    "            if not isfile(frame):\n",
    "                return None\n",
    "            window_fnames.append(frame)\n",
    "        return window_fnames\n",
    "\n",
    "    def read_window(self, window_fnames):\n",
    "        if window_fnames is None: return None\n",
    "        window = []\n",
    "        for fname in window_fnames:\n",
    "            img = cv2.imread(fname)\n",
    "            if img is None:\n",
    "                return None\n",
    "            try:\n",
    "                img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n",
    "            except Exception as e:\n",
    "                return None\n",
    "\n",
    "            window.append(img)\n",
    "\n",
    "        return window\n",
    "\n",
    "    def crop_audio_window(self, spec, start_frame):\n",
    "        if type(start_frame) == int:\n",
    "            start_frame_num = start_frame\n",
    "        else:\n",
    "            start_frame_num = self.get_frame_id(start_frame) # 0-indexing ---> 1-indexing\n",
    "        start_idx = int(80. * (start_frame_num / float(hparams.fps)))\n",
    "        \n",
    "        end_idx = start_idx + syncnet_mel_step_size\n",
    "\n",
    "        return spec[start_idx : end_idx, :]\n",
    "\n",
    "    def get_segmented_mels(self, spec, start_frame):\n",
    "        mels = []\n",
    "        assert syncnet_T == 5\n",
    "        start_frame_num = self.get_frame_id(start_frame) + 1 # 0-indexing ---> 1-indexing\n",
    "        if start_frame_num - 2 < 0: return None\n",
    "        for i in range(start_frame_num, start_frame_num + syncnet_T):\n",
    "            m = self.crop_audio_window(spec, i - 2)\n",
    "            if m.shape[0] != syncnet_mel_step_size:\n",
    "                return None\n",
    "            mels.append(m.T)\n",
    "\n",
    "        mels = np.asarray(mels)\n",
    "\n",
    "        return mels\n",
    "\n",
    "    def prepare_window(self, window):\n",
    "        # 3 x T x H x W\n",
    "        x = np.asarray(window) / 255.\n",
    "        x = np.transpose(x, (3, 0, 1, 2))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while 1:\n",
    "            idx = random.randint(0, len(self.all_videos) - 1) \n",
    "            vidname = self.all_videos[idx]\n",
    "            img_names = list(glob(join(vidname, '*.jpg')))\n",
    "            if len(img_names) <= 3 * syncnet_T:\n",
    "                continue\n",
    "            \n",
    "            img_name = random.choice(img_names)\n",
    "            wrong_img_name = random.choice(img_names)\n",
    "            while wrong_img_name == img_name:\n",
    "                wrong_img_name = random.choice(img_names)\n",
    "\n",
    "            window_fnames = self.get_window(img_name)\n",
    "            wrong_window_fnames = self.get_window(wrong_img_name)\n",
    "            if window_fnames is None or wrong_window_fnames is None:\n",
    "                continue\n",
    "\n",
    "            window = self.read_window(window_fnames)\n",
    "            if window is None:\n",
    "                continue\n",
    "\n",
    "            wrong_window = self.read_window(wrong_window_fnames)\n",
    "            if wrong_window is None:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                \n",
    "                wavpath = join(vidname, \"audio.wav\")\n",
    "                wav = audio.load_wav(wavpath, hparams.sample_rate)\n",
    "                #spectrogram\n",
    "                orig_mel = audio.melspectrogram(wav).T\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            # 分割 mel-spectrogram\n",
    "            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n",
    "            \n",
    "            if (mel.shape[0] != syncnet_mel_step_size):\n",
    "                continue\n",
    "\n",
    "            indiv_mels = self.get_segmented_mels(orig_mel.copy(), img_name)\n",
    "            if indiv_mels is None: continue\n",
    "\n",
    "            window = self.prepare_window(window)\n",
    "            y = window.copy()\n",
    "            window[:, :, window.shape[2]//2:] = 0.\n",
    "\n",
    "            wrong_window = self.prepare_window(wrong_window)\n",
    "            x = np.concatenate([window, wrong_window], axis=0)\n",
    "\n",
    "            x = torch.FloatTensor(x)\n",
    "            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n",
    "            indiv_mels = torch.FloatTensor(indiv_mels).unsqueeze(1)\n",
    "            y = torch.FloatTensor(y)\n",
    "            return x, indiv_mels, mel, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f39ed863",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T12:18:43.412702Z",
     "iopub.status.busy": "2023-11-02T12:18:43.412086Z",
     "iopub.status.idle": "2023-11-02T12:18:43.652798Z",
     "shell.execute_reply": "2023-11-02T12:18:43.650791Z",
     "shell.execute_reply.started": "2021-07-25T08:00:15.736819Z"
    },
    "papermill": {
     "duration": 13.106986,
     "end_time": "2023-11-02T12:18:43.653046",
     "exception": false,
     "start_time": "2023-11-02T12:18:30.546060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5, 96, 96])\n",
      "torch.Size([5, 1, 80, 16])\n",
      "torch.Size([1, 80, 16])\n",
      "torch.Size([3, 5, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "ds=Dataset(\"train\")\n",
    "x, indiv_mels, mel, y=ds[0]\n",
    "print(x.shape)\n",
    "print(indiv_mels.shape)\n",
    "print(mel.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93384d1c",
   "metadata": {
    "papermill": {
     "duration": 12.877973,
     "end_time": "2023-11-02T12:19:09.532109",
     "exception": false,
     "start_time": "2023-11-02T12:18:56.654136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81372502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T12:19:35.595362Z",
     "iopub.status.busy": "2023-11-02T12:19:35.574786Z",
     "iopub.status.idle": "2023-11-02T12:19:35.760657Z",
     "shell.execute_reply": "2023-11-02T12:19:35.759997Z",
     "shell.execute_reply.started": "2021-07-25T08:00:17.791703Z"
    },
    "papermill": {
     "duration": 13.307989,
     "end_time": "2023-11-02T12:19:35.760801",
     "exception": false,
     "start_time": "2023-11-02T12:19:22.452812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#bceloss\n",
    "logloss = nn.BCELoss()\n",
    "def cosine_loss(a, v, y):\n",
    "    d = nn.functional.cosine_similarity(a, v)\n",
    "    loss = logloss(d.unsqueeze(1), y)\n",
    "\n",
    "    return loss\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "syncnet = SyncNet().to(device) #  \n",
    "for p in syncnet.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "    \n",
    "#####L1 loss    \n",
    "recon_loss = nn.L1Loss()\n",
    "def get_sync_loss(mel, g):\n",
    "    g = g[:, :, :, g.size(3)//2:]\n",
    "    g = torch.cat([g[:, :, i] for i in range(syncnet_T)], dim=1)\n",
    "    # B, 3 * T, H//2, W\n",
    "    a, v = syncnet(mel, g)\n",
    "    y = torch.ones(g.size(0), 1).float().to(device)\n",
    "    return cosine_loss(a, v, y)\n",
    "\n",
    "def train(device, model, disc, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n",
    "    global global_step, global_epoch\n",
    "    resumed_step = global_step\n",
    "\n",
    "    while global_epoch < nepochs:\n",
    "        print('Starting Epoch: {}'.format(global_epoch))\n",
    "        running_sync_loss, running_l1_loss, disc_loss, running_perceptual_loss = 0., 0., 0., 0.\n",
    "        running_disc_real_loss, running_disc_fake_loss = 0., 0.\n",
    "        prog_bar = tqdm(enumerate(train_data_loader))\n",
    "        for step, (x, indiv_mels, mel, gt) in prog_bar:\n",
    "            disc.train()\n",
    "            model.train()\n",
    "\n",
    "            x = x.to(device)\n",
    "            mel = mel.to(device)\n",
    "            indiv_mels = indiv_mels.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            ### Train generator now. Remove ALL grads. \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            disc_optimizer.zero_grad()\n",
    "\n",
    "            g = model(indiv_mels, x)\n",
    "\n",
    "            if hparams.syncnet_wt > 0.:\n",
    "                sync_loss = get_sync_loss(mel, g) \n",
    "            else:\n",
    "                sync_loss = 0.\n",
    "\n",
    "            if hparams.disc_wt > 0.:\n",
    "                perceptual_loss = disc.perceptual_forward(g)\n",
    "            else:\n",
    "                perceptual_loss = 0.\n",
    "\n",
    "            l1loss = recon_loss(g, gt)#l1 loss，\n",
    "            \n",
    "            \n",
    "            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n",
    "                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ### Remove all gradients before Training disc\n",
    "            \n",
    "            disc_optimizer.zero_grad()\n",
    "\n",
    "            pred = disc(gt)\n",
    "            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n",
    "            disc_real_loss.backward()\n",
    "\n",
    "            pred = disc(g.detach())\n",
    "            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n",
    "            disc_fake_loss.backward()\n",
    "\n",
    "            disc_optimizer.step()\n",
    "\n",
    "            running_disc_real_loss += disc_real_loss.item()\n",
    "            running_disc_fake_loss += disc_fake_loss.item()\n",
    "\n",
    "            # Logs\n",
    "            global_step += 1\n",
    "            cur_session_steps = global_step - resumed_step\n",
    "\n",
    "            running_l1_loss += l1loss.item()\n",
    "            if hparams.syncnet_wt > 0.:\n",
    "                running_sync_loss += sync_loss.item()\n",
    "            else:\n",
    "                running_sync_loss += 0.\n",
    "\n",
    "            if hparams.disc_wt > 0.:\n",
    "                running_perceptual_loss += perceptual_loss.item()\n",
    "            else:\n",
    "                running_perceptual_loss += 0.\n",
    "\n",
    "            if global_step == 1 or global_step % checkpoint_interval == 0:\n",
    "                save_checkpoint(\n",
    "                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n",
    "                save_checkpoint(disc, disc_optimizer, global_step, checkpoint_dir, global_epoch, prefix='disc_')\n",
    "\n",
    "\n",
    "            if global_step % hparams.eval_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    average_sync_loss = eval_model(test_data_loader, global_step, device, model, disc)\n",
    "\n",
    "                    if average_sync_loss < .75:\n",
    "                        hparams.set_hparam('syncnet_wt', 0.03)\n",
    "\n",
    "            prog_bar.set_description('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(running_l1_loss / (step + 1),\n",
    "                                                                                        running_sync_loss / (step + 1),\n",
    "                                                                                        running_perceptual_loss / (step + 1),\n",
    "                                                                                        running_disc_fake_loss / (step + 1),\n",
    "                                                                                        running_disc_real_loss / (step + 1)))\n",
    "\n",
    "        global_epoch += 1\n",
    "\n",
    "def eval_model(test_data_loader, global_step, device, model, disc):\n",
    "    eval_steps = 300\n",
    "    print('Evaluating for {} steps'.format(eval_steps))\n",
    "    running_sync_loss, running_l1_loss, running_disc_real_loss, running_disc_fake_loss, running_perceptual_loss = [], [], [], [], []\n",
    "    while 1:\n",
    "        for step, (x, indiv_mels, mel, gt) in enumerate((test_data_loader)):\n",
    "            model.eval()\n",
    "            disc.eval()\n",
    "\n",
    "            x = x.to(device)\n",
    "            mel = mel.to(device)\n",
    "            indiv_mels = indiv_mels.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            pred = disc(gt)\n",
    "            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n",
    "\n",
    "            g = model(indiv_mels, x)\n",
    "            pred = disc(g)\n",
    "            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n",
    "\n",
    "            running_disc_real_loss.append(disc_real_loss.item())\n",
    "            running_disc_fake_loss.append(disc_fake_loss.item())\n",
    "\n",
    "            sync_loss = get_sync_loss(mel, g)\n",
    "            \n",
    "            if hparams.disc_wt > 0.:\n",
    "                perceptual_loss = disc.perceptual_forward(g)\n",
    "            else:\n",
    "                perceptual_loss = 0.\n",
    "\n",
    "            l1loss = recon_loss(g, gt)\n",
    "\n",
    "            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n",
    "                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n",
    "\n",
    "            running_l1_loss.append(l1loss.item())\n",
    "            running_sync_loss.append(sync_loss.item())\n",
    "            \n",
    "            if hparams.disc_wt > 0.:\n",
    "                running_perceptual_loss.append(perceptual_loss.item())\n",
    "            else:\n",
    "                running_perceptual_loss.append(0.)\n",
    "\n",
    "            if step > eval_steps: break\n",
    "\n",
    "        print('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(sum(running_l1_loss) / len(running_l1_loss),\n",
    "                                                            sum(running_sync_loss) / len(running_sync_loss),\n",
    "                                                            sum(running_perceptual_loss) / len(running_perceptual_loss),\n",
    "                                                            sum(running_disc_fake_loss) / len(running_disc_fake_loss),\n",
    "                                                             sum(running_disc_real_loss) / len(running_disc_real_loss)))\n",
    "        return sum(running_sync_loss) / len(running_sync_loss)\n",
    "\n",
    "latest_wav2lip_checkpoint = ''\n",
    "def save_checkpoint(model, optimizer, step, checkpoint_dir, epoch, prefix=''):\n",
    "    global latest_wav2lip_checkpoint\n",
    "    checkpoint_path = join(\n",
    "        checkpoint_dir, \"{}checkpoint_step{:09d}.pth\".format(prefix, global_step))\n",
    "    if 'disc' not in checkpoint_path:\n",
    "        latest_wav2lip_checkpoint = checkpoint_path\n",
    "    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n",
    "    torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer_state,\n",
    "        \"global_step\": step,\n",
    "        \"global_epoch\": epoch,\n",
    "    }, checkpoint_path)\n",
    "    print(\"Saved checkpoint:\", checkpoint_path)\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s)\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if overwrite_global_states:\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e868e5a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T12:20:01.679168Z",
     "iopub.status.busy": "2023-11-02T12:20:01.678315Z",
     "iopub.status.idle": "2023-11-02T17:21:51.147365Z",
     "shell.execute_reply": "2023-11-02T17:21:51.146775Z",
     "shell.execute_reply.started": "2021-07-25T08:01:08.453052Z"
    },
    "papermill": {
     "duration": 18122.354122,
     "end_time": "2023-11-02T17:21:51.147520",
     "exception": false,
     "start_time": "2023-11-02T12:19:48.793398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total trainable params 36298035\n",
      "total DISC trainable params 14113793\n",
      "Load checkpoint from: /kaggle/working/expert_checkpoints/checkpoint_step000080000.pth\n",
      "Starting Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/checkpoint_step000000001.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.2125028520822525, Sync: 0.0, Percep: 0.7112582325935364 | Fake: 0.6753584742546082, Real: 0.711258053779602: : 1it [00:14, 14.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/disc_checkpoint_step000000001.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.05340288229884797, Sync: 0.0, Percep: 0.7204441924746852 | Fake: 0.6801793053044074, Real: 0.6945417820647964: : 2865it [59:20,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "L1: 0.0413928986457524, Sync: 0.0, Percep: 0.7049959270811793 | Fake: 0.6840310145669909, Real: 0.6998657940038994: : 134it [02:59,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/checkpoint_step000003000.pth\n",
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/disc_checkpoint_step000003000.pth\n",
      "Evaluating for 300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.04138963855803013, Sync: 0.0, Percep: 0.7048521986714116 | Fake: 0.6841630224828367, Real: 0.6996343100512469: : 135it [03:43, 14.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1: 0.03474966962547863, Sync: 1.433393979773802, Percep: 0.6570781057371813 | Fake: 0.7350428858224083, Real: 0.6459384718362022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.039079156693519725, Sync: 0.0, Percep: 0.6948325128299404 | Fake: 0.6939024634178188, Real: 0.6914276647422118: : 2865it [59:55,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.03721257846206981, Sync: 0.0, Percep: 0.6973714775312345 | Fake: 0.6904829089083193, Real: 0.6949389966890271: : 269it [05:50,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/checkpoint_step000006000.pth\n",
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/disc_checkpoint_step000006000.pth\n",
      "Evaluating for 300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.03723369523606918, Sync: 0.0, Percep: 0.6972903196458463 | Fake: 0.6905608594417572, Real: 0.6948745882069624: : 270it [06:36, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1: 0.033142985995201504, Sync: 0.9321227468111936, Percep: 0.6870262044317582 | Fake: 0.699770409394713, Real: 0.6851317908834008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.03566748835926593, Sync: 0.0, Percep: 0.6969164271837338 | Fake: 0.6923279718526787, Real: 0.6925369480933611: : 2865it [1:00:45,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "L1: 0.03445471352021588, Sync: 0.0, Percep: 0.6955329941050841 | Fake: 0.6938766646798294, Real: 0.6892058345129585: : 404it [08:38,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/checkpoint_step000009000.pth\n",
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/disc_checkpoint_step000009000.pth\n",
      "Evaluating for 300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.03447484033711162, Sync: 0.0, Percep: 0.6955338844546566 | Fake: 0.6938694169491897, Real: 0.6891788571337123: : 405it [09:25, 15.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1: 0.03377383752890369, Sync: 0.8116050901658395, Percep: 0.6949812717297498 | Fake: 0.6921895707354826, Real: 0.6778730522183811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.03413254674231507, Sync: 0.0, Percep: 0.6988335271156271 | Fake: 0.6907379700355297, Real: 0.6931049331788617: : 2865it [1:00:37,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "L1: 0.032855503223076156, Sync: 0.0, Percep: 0.6984667263800669 | Fake: 0.6912781460165757, Real: 0.6921107965851537: : 539it [11:22,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/checkpoint_step000012000.pth\n",
      "Saved checkpoint: /kaggle/working/wav2lip_checkpoints/disc_checkpoint_step000012000.pth\n",
      "Evaluating for 300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.03286198592503314, Sync: 0.0, Percep: 0.6985279241093882 | Fake: 0.6912137924521058, Real: 0.6921684326948943: : 540it [12:07, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1: 0.027995452723082376, Sync: 0.706808291375637, Percep: 0.7177062884849661 | Fake: 0.670196591054692, Real: 0.7052458568530924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L1: 0.03382600274450596, Sync: 0.15704056348320078, Percep: 0.7008238289862819 | Fake: 0.6898790033075822, Real: 0.693933175199943: : 2865it [1:01:07,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"/kaggle/working/wav2lip_checkpoints\"  #checkpoint 存储的位置\n",
    "\n",
    "# Dataset and Dataloader setup\n",
    "train_dataset = Dataset('train')\n",
    "test_dataset = Dataset('val')\n",
    "\n",
    "train_data_loader = data_utils.DataLoader(\n",
    "    train_dataset, batch_size=hparams.batch_size, shuffle=True,\n",
    "    num_workers=hparams.num_workers)\n",
    "\n",
    "test_data_loader = data_utils.DataLoader(\n",
    "    test_dataset, batch_size=hparams.batch_size,\n",
    "    num_workers=4)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    " # Model\n",
    "model = Wav2Lip().to(device)\n",
    "disc = Wav2Lip_disc_qual().to(device) \n",
    "\n",
    "print('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "print('total DISC trainable params {}'.format(sum(p.numel() for p in disc.parameters() if p.requires_grad)))\n",
    "\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                       lr=hparams.initial_learning_rate,\n",
    "                       betas=(0.5, 0.999))#####adam，betas=[0.5,0.999]\n",
    "disc_optimizer = optim.Adam([p for p in disc.parameters() if p.requires_grad],\n",
    "                            lr=hparams.disc_initial_learning_rate,\n",
    "                            betas=(0.5, 0.999))#####adam，betas=[0.5,0.999]\n",
    "\n",
    "\n",
    "# checkpoint_path=\"\"\n",
    "# load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=False)\n",
    "# disc_checkpoint_path=\"\"\n",
    "# load_checkpoint(disc_checkpoint_path, disc, disc_optimizer, \n",
    "#                             reset_optimizer=False, overwrite_global_states=False)\n",
    "\n",
    "syncnet_checkpoint_path = latest_checkpoint_path\n",
    "# syncnet_checkpoint_path=\"/kaggle/working/expert_checkpoints/checkpoint_step000000001.pth\"\n",
    "load_checkpoint(syncnet_checkpoint_path, syncnet, None, reset_optimizer=True,\n",
    "                            overwrite_global_states=False)\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "\n",
    "# Train!\n",
    "train(device, model, disc, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n",
    "          checkpoint_dir=checkpoint_dir,\n",
    "          checkpoint_interval=hparams.checkpoint_interval,\n",
    "          nepochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2e26081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T17:22:35.806916Z",
     "iopub.status.busy": "2023-11-02T17:22:35.805846Z",
     "iopub.status.idle": "2023-11-02T17:22:35.809077Z",
     "shell.execute_reply": "2023-11-02T17:22:35.808212Z"
    },
    "papermill": {
     "duration": 22.313437,
     "end_time": "2023-11-02T17:22:35.809261",
     "exception": false,
     "start_time": "2023-11-02T17:22:13.495824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python wav2lip_train.py --data_root lrs2_preprocessed/ --checkpoint_dir <folder_to_save_checkpoints> --syncnet_checkpoint_path <path_to_expert_disc_checkpoint>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7eac30f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T17:23:20.112491Z",
     "iopub.status.busy": "2023-11-02T17:23:20.111792Z",
     "iopub.status.idle": "2023-11-02T17:23:20.158825Z",
     "shell.execute_reply": "2023-11-02T17:23:20.159303Z",
     "shell.execute_reply.started": "2021-07-24T13:57:11.710238Z"
    },
    "papermill": {
     "duration": 22.179579,
     "end_time": "2023-11-02T17:23:20.159471",
     "exception": false,
     "start_time": "2023-11-02T17:22:57.979892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import listdir, path\n",
    "import numpy as np\n",
    "import scipy, cv2, os, sys, argparse, audio\n",
    "import json, subprocess, random, string\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import torch, face_detection\n",
    "from models import Wav2Lip\n",
    "import platform\n",
    "import audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d4db1b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T17:24:04.815366Z",
     "iopub.status.busy": "2023-11-02T17:24:04.814696Z",
     "iopub.status.idle": "2023-11-02T17:24:04.817593Z",
     "shell.execute_reply": "2023-11-02T17:24:04.816996Z",
     "shell.execute_reply.started": "2021-07-24T13:59:02.638662Z"
    },
    "papermill": {
     "duration": 22.31943,
     "end_time": "2023-11-02T17:24:04.817753",
     "exception": false,
     "start_time": "2023-11-02T17:23:42.498323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_path=\"/kaggle/working/wav2lip_checkpoints/checkpoint_step000000001.pth\"#生成器的checkpoint位置\n",
    "checkpoint_path = latest_wav2lip_checkpoint\n",
    "face=\"/kaggle/input/jakeydata/jakedrive.mp4\" \n",
    "speech=\"/kaggle/input/jakeydata/laucky 2.wav\"\n",
    "resize_factor=1 \n",
    "crop=[0,-1,0,-1] \n",
    "fps=25\n",
    "static=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3b7fa1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T17:24:49.196949Z",
     "iopub.status.busy": "2023-11-02T17:24:49.196319Z",
     "iopub.status.idle": "2023-11-02T17:24:49.333289Z",
     "shell.execute_reply": "2023-11-02T17:24:49.332680Z",
     "shell.execute_reply.started": "2021-07-24T13:59:14.496913Z"
    },
    "papermill": {
     "duration": 22.426508,
     "end_time": "2023-11-02T17:24:49.333430",
     "exception": false,
     "start_time": "2023-11-02T17:24:26.906922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading video frames...\n",
      "Number of frames available for inference: 169\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(face):\n",
    "    raise ValueError('--face argument must be a valid path to video/image file')\n",
    "\n",
    "\n",
    "else:# 若输入的是视频格式\n",
    "    video_stream = cv2.VideoCapture(face)# 读取视频\n",
    "    fps = video_stream.get(cv2.CAP_PROP_FPS)# 读取 fps\n",
    "\n",
    "    print('Reading video frames...')\n",
    "\n",
    "    full_frames = []\n",
    "    #提取所有的帧\n",
    "    while 1:\n",
    "        still_reading, frame = video_stream.read()\n",
    "        if not still_reading:\n",
    "            video_stream.release()\n",
    "            break\n",
    "        if resize_factor > 1: # 进行下采样，降低分辨率\n",
    "            frame = cv2.resize(frame, (frame.shape[1]//resize_factor, frame.shape[0]//resize_factor))\n",
    "\n",
    "        \n",
    "\n",
    "        y1, y2, x1, x2 =crop  # 裁剪\n",
    "        if x2 == -1: x2 = frame.shape[1]\n",
    "        if y2 == -1: y2 = frame.shape[0]\n",
    "\n",
    "        frame = frame[y1:y2, x1:x2]\n",
    "\n",
    "        full_frames.append(frame)\n",
    "\n",
    "print (\"Number of frames available for inference: \"+str(len(full_frames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7a6ce0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T17:25:33.739434Z",
     "iopub.status.busy": "2023-11-02T17:25:33.738776Z",
     "iopub.status.idle": "2023-11-02T17:25:34.540425Z",
     "shell.execute_reply": "2023-11-02T17:25:34.541324Z",
     "shell.execute_reply.started": "2021-07-24T13:59:29.319841Z"
    },
    "papermill": {
     "duration": 23.034511,
     "end_time": "2023-11-02T17:25:34.541660",
     "exception": false,
     "start_time": "2023-11-02T17:25:11.507149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 493)\n"
     ]
    }
   ],
   "source": [
    "#检查输入的音频是否为 .wav格式的，若不是则进行转换\n",
    "if not speech.endswith('.wav'):\n",
    "    print('Extracting raw audio...')\n",
    "    command = 'ffmpeg -y -i {} -strict -2 {}'.format(speech, 'temp/temp.wav')\n",
    "\n",
    "    subprocess.call(command, shell=True)\n",
    "    speech = 'temp/temp.wav'\n",
    "\n",
    "wav = audio.load_wav(speech, 16000)#保证采样率为16000\n",
    "mel = audio.melspectrogram(wav)\n",
    "print(mel.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8edadf0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T17:26:19.369176Z",
     "iopub.status.busy": "2023-11-02T17:26:19.368476Z",
     "iopub.status.idle": "2023-11-02T17:26:19.371956Z",
     "shell.execute_reply": "2023-11-02T17:26:19.371433Z",
     "shell.execute_reply.started": "2021-07-24T13:59:40.598345Z"
    },
    "papermill": {
     "duration": 22.307306,
     "end_time": "2023-11-02T17:26:19.372107",
     "exception": false,
     "start_time": "2023-11-02T17:25:57.064801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of mel chunks: 151\n"
     ]
    }
   ],
   "source": [
    "wav2lip_batch_size=128 \n",
    "mel_step_size=16\n",
    "\n",
    "mel_chunks = []\n",
    "mel_idx_multiplier = 80./fps \n",
    "i = 0\n",
    "while 1:\n",
    "    start_idx = int(i * mel_idx_multiplier)\n",
    "    if start_idx + mel_step_size > len(mel[0]):\n",
    "        mel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])\n",
    "        break\n",
    "    mel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])\n",
    "    i += 1\n",
    "\n",
    "print(\"Length of mel chunks: {}\".format(len(mel_chunks)))\n",
    "\n",
    "full_frames = full_frames[:len(mel_chunks)]\n",
    "\n",
    "batch_size = wav2lip_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3443d1d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T17:27:03.756685Z",
     "iopub.status.busy": "2023-11-02T17:27:03.755732Z",
     "iopub.status.idle": "2023-11-02T17:27:03.759386Z",
     "shell.execute_reply": "2023-11-02T17:27:03.758875Z",
     "shell.execute_reply.started": "2021-07-24T13:59:59.813928Z"
    },
    "papermill": {
     "duration": 22.125607,
     "end_time": "2023-11-02T17:27:03.759515",
     "exception": false,
     "start_time": "2023-11-02T17:26:41.633908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference.\n"
     ]
    }
   ],
   "source": [
    "img_size = 96 \n",
    "pads=[0,20,0,0] \n",
    "nosmooth=False\n",
    "face_det_batch_size=16\n",
    "\n",
    "def get_smoothened_boxes(boxes, T):\n",
    "    for i in range(len(boxes)):\n",
    "        if i + T > len(boxes):\n",
    "            window = boxes[len(boxes) - T:]\n",
    "        else:\n",
    "            window = boxes[i : i + T]\n",
    "        boxes[i] = np.mean(window, axis=0)\n",
    "    return boxes\n",
    "\n",
    "#人脸检测函数\n",
    "def face_detect(images):\n",
    "    detector = face_detection.FaceAlignment(face_detection.LandmarksType._2D, \n",
    "                                            flip_input=False, device=device)\n",
    "\n",
    "    batch_size = face_det_batch_size\n",
    "\n",
    "    while 1:\n",
    "        predictions = []\n",
    "        try:\n",
    "            for i in tqdm(range(0, len(images), batch_size)):\n",
    "                predictions.extend(detector.get_detections_for_batch(np.array(images[i:i + batch_size])))\n",
    "        except RuntimeError:\n",
    "            if batch_size == 1: \n",
    "                raise RuntimeError('Image too big to run face detection on GPU. Please use the --resize_factor argument')\n",
    "            batch_size //= 2\n",
    "            print('Recovering from OOM error; New batch size: {}'.format(batch_size))\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    results = []\n",
    "    pady1, pady2, padx1, padx2 = pads\n",
    "    for rect, image in zip(predictions, images):\n",
    "        if rect is None:\n",
    "            cv2.imwrite('temp/faulty_frame.jpg', image) # check this frame where the face was not detected.\n",
    "            raise ValueError('Face not detected! Ensure the video contains a face in all the frames.')\n",
    "\n",
    "        y1 = max(0, rect[1] - pady1)\n",
    "        y2 = min(image.shape[0], rect[3] + pady2)\n",
    "        x1 = max(0, rect[0] - padx1)\n",
    "        x2 = min(image.shape[1], rect[2] + padx2)\n",
    "\n",
    "        results.append([x1, y1, x2, y2])\n",
    "\n",
    "    boxes = np.array(results)\n",
    "    if not nosmooth: boxes = get_smoothened_boxes(boxes, T=5)\n",
    "    results = [[image[y1: y2, x1:x2], (y1, y2, x1, x2)] for image, (x1, y1, x2, y2) in zip(images, boxes)]\n",
    "\n",
    "    del detector\n",
    "    return results \n",
    "\n",
    "box=[-1,-1,-1,-1]\n",
    "\n",
    "def datagen(frames, mels):\n",
    "    img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
    "\n",
    "    if box[0] == -1:# 如果未指定 特定的人脸边界的话\n",
    "        if not static:# 是否使用视频的第一帧作为参考\n",
    "            face_det_results = face_detect(frames) # BGR2RGB for CNN face detection\n",
    "        else:\n",
    "            face_det_results = face_detect([frames[0]])\n",
    "    else:\n",
    "        print('Using the specified bounding box instead of face detection...')\n",
    "        y1, y2, x1, x2 = box\n",
    "        face_det_results = [[f[y1: y2, x1:x2], (y1, y2, x1, x2)] for f in frames] # 裁剪出人脸结果\n",
    "\n",
    "    for i, m in enumerate(mels):\n",
    "        idx = 0 if static else i%len(frames)\n",
    "        frame_to_save = frames[idx].copy()\n",
    "        face, coords = face_det_results[idx].copy()\n",
    "\n",
    "        face = cv2.resize(face, (img_size, img_size)) # 重采样到指定大小\n",
    "\n",
    "        img_batch.append(face)\n",
    "        mel_batch.append(m)\n",
    "        frame_batch.append(frame_to_save)\n",
    "        coords_batch.append(coords)\n",
    "\n",
    "        if len(img_batch) >= wav2lip_batch_size:\n",
    "            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
    "\n",
    "            img_masked = img_batch.copy()\n",
    "            img_masked[:, img_size//2:] = 0\n",
    "\n",
    "            img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
    "            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
    "\n",
    "            yield img_batch, mel_batch, frame_batch, coords_batch\n",
    "            img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n",
    "\n",
    "    if len(img_batch) > 0:\n",
    "        img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n",
    "\n",
    "        img_masked = img_batch.copy()\n",
    "        img_masked[:, img_size//2:] = 0\n",
    "\n",
    "        img_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n",
    "        mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
    "\n",
    "        yield img_batch, mel_batch, frame_batch, coords_batch\n",
    "\n",
    "mel_step_size = 16 \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} for inference.'.format(device))\n",
    "\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    if device == 'cuda':\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint\n",
    "\n",
    "def load_model(path):\n",
    "    model = Wav2Lip()\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s)\n",
    "\n",
    "    model = model.to(device)\n",
    "    return model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "752cf082",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T17:27:48.239326Z",
     "iopub.status.busy": "2023-11-02T17:27:48.238725Z",
     "iopub.status.idle": "2023-11-02T17:27:48.241676Z",
     "shell.execute_reply": "2023-11-02T17:27:48.241149Z"
    },
    "papermill": {
     "duration": 22.32066,
     "end_time": "2023-11-02T17:27:48.241805",
     "exception": false,
     "start_time": "2023-11-02T17:27:25.921145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.mkdir('/kaggle/working/temp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3690d6d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T17:28:32.544447Z",
     "iopub.status.busy": "2023-11-02T17:28:32.543798Z",
     "iopub.status.idle": "2023-11-02T17:28:39.470533Z",
     "shell.execute_reply": "2023-11-02T17:28:39.470050Z",
     "shell.execute_reply.started": "2021-07-24T14:00:13.510341Z"
    },
    "papermill": {
     "duration": 29.148233,
     "end_time": "2023-11-02T17:28:39.470686",
     "exception": false,
     "start_time": "2023-11-02T17:28:10.322453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:00<00:07,  1.13it/s]\u001b[A\n",
      " 20%|██        | 2/10 [00:01<00:04,  1.93it/s]\u001b[A\n",
      " 30%|███       | 3/10 [00:01<00:02,  2.50it/s]\u001b[A\n",
      " 40%|████      | 4/10 [00:01<00:02,  2.91it/s]\u001b[A\n",
      " 50%|█████     | 5/10 [00:01<00:01,  3.21it/s]\u001b[A\n",
      " 60%|██████    | 6/10 [00:02<00:01,  3.41it/s]\u001b[A\n",
      " 70%|███████   | 7/10 [00:02<00:00,  3.54it/s]\u001b[A\n",
      " 80%|████████  | 8/10 [00:02<00:00,  3.67it/s]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:02<00:00,  3.76it/s]\u001b[A\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint from: /kaggle/working/wav2lip_checkpoints/checkpoint_step000012000.pth\n",
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.46s/it]\n"
     ]
    }
   ],
   "source": [
    "full_frames = full_frames[:len(mel_chunks)]\n",
    "\n",
    "batch_size = wav2lip_batch_size\n",
    "gen = datagen(full_frames.copy(), mel_chunks)  \n",
    "\n",
    "for i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(gen, \n",
    "                                        total=int(np.ceil(float(len(mel_chunks))/batch_size)))):\n",
    "    \n",
    "    if i == 0:\n",
    "        model = load_model(checkpoint_path)\n",
    "        print (\"Model loaded\")\n",
    "\n",
    "        frame_h, frame_w = full_frames[0].shape[:-1]\n",
    "        \n",
    "        out = cv2.VideoWriter('/kaggle/working/temp/result_without_audio.mp4',\n",
    "                                cv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_w, frame_h))\n",
    "\n",
    "    img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)\n",
    "    mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)\n",
    "    \n",
    "    \n",
    "    # img_batch,\n",
    "    ##############TODO\n",
    "    with torch.no_grad():\n",
    "        pred = model(mel_batch, img_batch)\n",
    "    \n",
    "    pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n",
    "\n",
    "    for p, f, c in zip(pred, frames, coords):\n",
    "        y1, y2, x1, x2 = c\n",
    "        p = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))\n",
    "\n",
    "        f[y1:y2, x1:x2] = p\n",
    "        out.write(f)\n",
    "\n",
    "out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e87b4e9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T17:29:24.260812Z",
     "iopub.status.busy": "2023-11-02T17:29:24.259853Z",
     "iopub.status.idle": "2023-11-02T17:29:24.262723Z",
     "shell.execute_reply": "2023-11-02T17:29:24.262118Z"
    },
    "papermill": {
     "duration": 22.448849,
     "end_time": "2023-11-02T17:29:24.262850",
     "exception": false,
     "start_time": "2023-11-02T17:29:01.814001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.mkdir('/kaggle/working/result/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80d4b122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-02T17:30:08.684434Z",
     "iopub.status.busy": "2023-11-02T17:30:08.683793Z",
     "iopub.status.idle": "2023-11-02T17:30:08.869671Z",
     "shell.execute_reply": "2023-11-02T17:30:08.869034Z",
     "shell.execute_reply.started": "2021-07-24T14:01:22.990563Z"
    },
    "papermill": {
     "duration": 22.326699,
     "end_time": "2023-11-02T17:30:08.869811",
     "exception": false,
     "start_time": "2023-11-02T17:29:46.543112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outfile=\"/kaggle/working/result/result.mp4\"\n",
    "command = 'ffmpeg -y -i {} -i {} -strict -2 -q:v 1 {}'.format(speech, '/kaggle/working/temp/result_without_audio.mp4',outfile)\n",
    "subprocess.call(command, shell=platform.system() != 'Windows')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27107.567313,
   "end_time": "2023-11-02T17:30:33.848811",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-02T09:58:46.281498",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
